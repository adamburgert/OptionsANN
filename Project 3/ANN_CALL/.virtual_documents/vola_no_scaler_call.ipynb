
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import random


Data_for_Call_options = pd.read_excel('train_gld_before_2016.xlsx', sheet_name='GLD_CALL')



seed = 42
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Extract date and volatility information
exp_dates_for_Call = Data_for_Call_options['Exp Date = T']
dates_for_Call = Data_for_Call_options['T - 5']
volas_for_Call_percent = Data_for_Call_options['21_Day_Percent_Volatility']
volas_for_Call_nominal = Data_for_Call_options['21_Day_Volatility']

# Prepare Features and Target
# Extract ETF price and strike columns as features
Features_for_Call_options = Data_for_Call_options[['ETF Price on T - 5', 'Strike_1', 'Strike_2', 'Strike_3', 'Strike_4', 'Strike_5','21_Day_Percent_Volatility','21_Day_Volatility']].copy()

# Extract each close price column as separate target for each strike
Close_prices_Call = Data_for_Call_options[['Close_1', 'Close_2', 'Close_3', 'Close_4', 'Close_5']]

# Convert columns to numeric (important if data contains any non-numeric entries)
Features_for_Call_options['ETF Price on T - 5'] = pd.to_numeric(Features_for_Call_options['ETF Price on T - 5'], errors='coerce')

# Drop rows with missing values in the feature set
Features_for_Call_options = Features_for_Call_options.dropna(subset=['ETF Price on T - 5'])

# Ensure the target aligns with the filtered features
Close_prices_Call = Close_prices_Call.loc[Features_for_Call_options.index]

# Check if CUDA is available and set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Tensor conversion for features and targets, with CUDA support
Call_features_tensor = torch.tensor(Features_for_Call_options.values, dtype=torch.float32).to(device)
Call_target_tensor = torch.tensor(Close_prices_Call.values, dtype=torch.float32).to(device)

# Optional: Check the shapes of the tensors for verification
print("Features tensor shape:", Call_features_tensor.shape)
print("Target tensor shape:", Call_target_tensor.shape)

# Model hyperparameters
lr = 0.001

import torch.nn as nn

class MultiOutputOptionsPricingANN(nn.Module):
    def __init__(self, InCall_Param, OutCall_param=100):
        super(MultiOutputOptionsPricingANN, self).__init__()

        hidden_neurons =  500  # neurons per hidden layer

        # Define hidden layers with batch norm
        self.L1 = nn.Linear(InCall_Param, hidden_neurons)
        self.batch_norm1 = nn.BatchNorm1d(hidden_neurons)

        self.L2 = nn.Linear(hidden_neurons, hidden_neurons)
        self.batch_norm2 = nn.BatchNorm1d(hidden_neurons)

        self.L3 = nn.Linear(hidden_neurons, hidden_neurons)
        self.batch_norm3 = nn.BatchNorm1d(hidden_neurons)

        self.L4 = nn.Linear(hidden_neurons, hidden_neurons)
        self.batch_norm4 = nn.BatchNorm1d(hidden_neurons)

        self.L5 = nn.Linear(hidden_neurons, hidden_neurons)
        self.batch_norm5 = nn.BatchNorm1d(hidden_neurons)

        self.L6 = nn.Linear(hidden_neurons, hidden_neurons)
        self.batch_norm6 = nn.BatchNorm1d(hidden_neurons)

        self.L7 = nn.Linear(hidden_neurons, hidden_neurons)
        self.batch_norm7 = nn.BatchNorm1d(hidden_neurons)

        # Output layer
        self.output_layer = nn.Linear(hidden_neurons, OutCall_param)

        # Activation function
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.batch_norm1(self.L1(x)))
        x = self.relu(self.batch_norm2(self.L2(x)))
        x = self.relu(self.batch_norm3(self.L3(x)))
        x = self.relu(self.batch_norm4(self.L4(x)))
        x = self.relu(self.batch_norm5(self.L5(x)))
        x = self.relu(self.batch_norm6(self.L6(x)))
        x = self.relu(self.batch_norm7(self.L7(x)))
        x = self.output_layer(x)  
        return x

# Initialize model and move it to the GPU
input_features = Features_for_Call_options.shape[1]
output_strikes = Close_prices_Call.shape[1]
Model_Call_options = MultiOutputOptionsPricingANN(input_features, output_strikes).to(device)

# Verify output shape
sample_input = torch.randn(5, input_features).to(device)  # Batch of 5 samples on the GPU
output = Model_Call_options(sample_input)
print("Model output shape:", output.shape)  # Expect (5, 5) for 5

# Define loss and optimizer
Call_options_Criteria = nn.MSELoss()  # Mean Squared Error Loss
Optimizer_for_Calls = optim.Adam(Model_Call_options.parameters(), lr=lr)

# Training loop for Call options
Num_Epochs = 2000
for Epoch in range(Num_Epochs):
    Model_Call_options.train()  # Set model to training mode
    
    Optimizer_for_Calls.zero_grad()
    
    # Forward pass: get predictions for all strike prices
    Target_out_Call = Model_Call_options(Call_features_tensor)
    
    # Compute loss
    Loss_for_Call_options = Call_options_Criteria(Target_out_Call, Call_target_tensor)
    
    # Backpropagation and optimization
    Loss_for_Call_options.backward()
    Optimizer_for_Calls.step()

    # Logging
    if (Epoch + 1) % 100 == 0:
        print(f'Epoch [{Epoch+1}/{Num_Epochs}], Loss (Call): {Loss_for_Call_options.item():.4f}')

# Prediction and evaluation with CUDA support
with torch.no_grad():
    Model_Call_options.eval()
    train_predictions_Call = Model_Call_options(Call_features_tensor).cpu().numpy()  # Move predictions back to CPU for evaluation

# Save the model
torch.save(Model_Call_options.state_dict(), "./training_results/call_option_model.pth")


# Metric Calculation
def Metrics(actual, predicted):
    # Calculate metrics for each strike price
    MSE = np.mean((actual - predicted) ** 2, axis=0)  # MSE for each strike
    MAE = np.mean(np.abs(actual - predicted), axis=0)  # MAE for each strike
    MAPE = np.mean(np.abs((actual - predicted) / actual), axis=0) * 100  # MAPE for each strike
    return MSE, MAE, MAPE

# Calculate metrics for training predictions
MSE_Call, MAE_Call, MAPE_Call = Metrics(Close_prices_Call.values, train_predictions_Call)  # Use the correct target
for i in range(len(MSE_Call)):
    print(f'Call Options Price {i+1}- Strike {i+1}: MSE (Training): {MSE_Call[i]:.4f}, MAE (Training): {MAE_Call[i]:.4f}, MAPE (Training): {MAPE_Call[i]:.4f}%')



# Create the Res_Call_options DataFrame
Res_Call_options = pd.DataFrame({
    'Exp Date = T': exp_dates_for_Call,
    'T - 5': dates_for_Call,
    '21_Day_Percent_Volatility': volas_for_Call_percent,
    '21_Day_Volatility': volas_for_Call_nominal,
    'ETF Price on T - 5': Features_for_Call_options['ETF Price on T - 5'].values,
    
    'Strike_1': Features_for_Call_options['Strike_1'].values,
    'Close_1': Data_for_Call_options['Close_1'].values,  # Actual close prices
    'Predicted Price_1': train_predictions_Call[:, 0],  # Predicted prices
    
    'Strike_2': Features_for_Call_options['Strike_2'].values,
    'Close_2': Data_for_Call_options['Close_2'].values,
    'Predicted Price_2': train_predictions_Call[:, 1],
    
    'Strike_3': Features_for_Call_options['Strike_3'].values,
    'Close_3': Data_for_Call_options['Close_3'].values,
    'Predicted Price_3': train_predictions_Call[:, 2],
    
    'Strike_4': Features_for_Call_options['Strike_4'].values,
    'Close_4': Data_for_Call_options['Close_4'].values,
    'Predicted Price_4': train_predictions_Call[:, 3],
    
    'Strike_5': Features_for_Call_options['Strike_5'].values,
    'Close_5': Data_for_Call_options['Close_5'].values,
    'Predicted Price_5': train_predictions_Call[:, 4],
    
})
pd.set_option('display.float_format', '{:.10f}'.format)

# Calculate the prediction differences and percentage differences for each strike
for i in range(1, 6):
    # Calculate the difference between predicted and actual close prices
    Res_Call_options[f'Price Prediction Difference_{i}'] = Res_Call_options[f'Predicted Price_{i}'] - Res_Call_options[f'Close_{i}']
    
    # Calculate the percentage difference
    Res_Call_options[f'Price Prediction Difference in percentage_{i}'] = (
        Res_Call_options[f'Price Prediction Difference_{i}'] / Res_Call_options[f'Close_{i}'] * 100
    )

# Average Price Prediction Difference calculations for each strike
Res_Call_options['Average Price Prediction Difference'] = Res_Call_options[
    [f'Price Prediction Difference_{i}' for i in range(1, 6)]
].mean(axis=1)

# Average Price Prediction Difference in percentage calculations for each strike
Res_Call_options['Average Price Prediction Difference in percentage'] = Res_Call_options[
    [f'Price Prediction Difference in percentage_{i}' for i in range(1, 6)]
].mean(axis=1)

# Overall Average Price Prediction Difference across all strikes
Res_Call_options['Overall Average Price Prediction Difference'] = Res_Call_options[
    [f'Price Prediction Difference_{i}' for i in range(1, 6)]
].mean(axis=1)

# Overall Average Price Prediction Difference in percentage across all strikes
Res_Call_options['Overall Average Price Prediction Difference in percentage'] = Res_Call_options[
    [f'Price Prediction Difference in percentage_{i}' for i in range(1, 6)]
].mean(axis=1)

# Add the metrics to the final DataFrame for each strike in Calls
for i in range(1, 6):  # Assuming there are 5 strike prices for Calls as well
    Res_Call_options[f'MSE_Call_{i}'] = MSE_Call[i - 1]  # Assign MSE for each strike
    Res_Call_options[f'MAE_Call_{i}'] = MAE_Call[i - 1]  # Assign MAE for each strike
    Res_Call_options[f'MAPE_Call_{i}'] = MAPE_Call[i - 1]  # Assign MAPE for each strike

# Calculate overall performance metrics based on close prices
MSE_Call = ((Res_Call_options[[f'Close_{i}' for i in range(1, 6)]].values - 
              Res_Call_options[[f'Predicted Price_{i}' for i in range(1, 6)]].values) ** 2).mean().mean()
MAE_Call = (abs(Res_Call_options[[f'Close_{i}' for i in range(1, 6)]].values - 
             Res_Call_options[[f'Predicted Price_{i}' for i in range(1, 6)]].values)).mean().mean()
MAPE_Call = (100 * abs((Res_Call_options[[f'Close_{i}' for i in range(1, 6)]].values - 
               Res_Call_options[[f'Predicted Price_{i}' for i in range(1, 6)]].values) / 
               Res_Call_options[[f'Close_{i}' for i in range(1, 6)]].values)).mean().mean()

# Assign overall model performance metrics to all rows
Res_Call_options['MSE'] = [MSE_Call] * len(Res_Call_options)  # Overall MSE based on close prices
Res_Call_options['MAE'] = [MAE_Call] * len(Res_Call_options)  # Overall MAE based on close prices
Res_Call_options['MAPE'] = [MAPE_Call] * len(Res_Call_options)  # Overall MAPE based on close prices


# Print the DataFrame and metrics for verification
print(Res_Call_options.head())
print(f'MSE: {MSE_Call}, MAE: {MAE_Call}, MAPE: {MAPE_Call}')

# Calculate the metrics for each strike price's close prediction for Calls
MSE_Call, MAE_Call, MAPE_Call = Metrics(Close_prices_Call.values, train_predictions_Call)

#Res_Call_options['Weight decay'] = weight_decay
Res_Call_options['Learning rate'] = lr
#Res_Call_options['Dropout rate'] = dropout
Res_Call_options['Epoch size'] = Num_Epochs
Res_Call_options['Random seed'] = seed


# Print out the metrics for each close price in Calls
for i in range(len(MSE_Call)):
    print(f'Call Options Price_{i+1} - Strike_{i+1}: MSE (Training): {MSE_Call[i]:.4f}, MAE (Training): {MAE_Call[i]:.4f}, MAPE (Training): {MAPE_Call[i]:.4f}%')


    

# Display the final DataFrame for Calls with the new metrics columns
print(Res_Call_options.head())





# Save results to Excel
with pd.ExcelWriter('./training_results/GLD_training_Adam_ANN_with_multiple_strikes_without_volatility_input_Calls_2000.xlsx') as writer:
    Res_Call_options.to_excel(writer, sheet_name='Call_Options', index=False)






import pandas as pd
import numpy as np
import torch

# Load testing data
testing_Data_for_Call_options = pd.read_excel('test_gld_after_2016.xlsx', sheet_name='GLD_CALL')
test_volas_for_Call_percent = testing_Data_for_Call_options['21_Day_Percent_Volatility']
test_volas_for_Call_nominal = testing_Data_for_Call_options['21_Day_Volatility']

# Clean column names
testing_Data_for_Call_options.columns = testing_Data_for_Call_options.columns.str.strip()

# Prepare features and targets for Call options
testing_Features_for_Call_options = testing_Data_for_Call_options[['ETF Price on T - 5', 'Strike_1', 'Strike_2', 'Strike_3', 'Strike_4', 'Strike_5','21_Day_Percent_Volatility','21_Day_Volatility']].copy()
testing_Data_Target_Call = testing_Data_for_Call_options[['Close_1', 'Close_2', 'Close_3', 'Close_4', 'Close_5']]

# Convert columns to numeric
testing_Features_for_Call_options['ETF Price on T - 5'] = pd.to_numeric(testing_Features_for_Call_options['ETF Price on T - 5'], errors='coerce')

# Drop rows with missing values in the feature set
testing_Features_for_Call_options = testing_Features_for_Call_options.dropna(subset=['ETF Price on T - 5'])

# Ensure the target aligns with the filtered features
test_Close_prices_Call = testing_Data_Target_Call.loc[testing_Features_for_Call_options.index]

# Get test dates and volatility
test_dates_for_Call = testing_Data_for_Call_options['T - 5']
exp_dates_for_Call_test = testing_Data_for_Call_options['Exp Date = T']

# Drop 'Exp Date = T' column for scaling and prediction
testing_Data_for_Call_options = testing_Data_for_Call_options.drop(columns=['Exp Date = T'])



# Check if CUDA is available and set the device (reuse if defined previously)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Tensor conversion for testing features and targets, with CUDA support
Call_features_tensor_test = torch.tensor(testing_Features_for_Call_options.values, dtype=torch.float32).to(device)
Call_target_tensor_test = torch.tensor(testing_Data_Target_Call.values, dtype=torch.float32).to(device)

# Load the model and move it to the GPU
model_Call_loaded = MultiOutputOptionsPricingANN(input_features, output_strikes).to(device)
model_Call_loaded.load_state_dict(torch.load('./training_results/call_option_model.pth'))
model_Call_loaded.eval()  # Set the model to evaluation mode

# Make predictions with the loaded Call options model
with torch.no_grad():  # Disable gradient calculation
    test_predictions_Call = model_Call_loaded(Call_features_tensor_test)

# Move predictions back to CPU and convert to numpy for further processing if needed
test_predictions_Call = test_predictions_Call.cpu().numpy()

# Ensure both actual and predicted arrays have the same shape
assert Call_target_tensor_test.shape == test_predictions_Call.shape, "Shape mismatch between actual and predicted prices."

# Function to calculate metrics
def Metrics(actual, predicted):
    MSE_test_Call = np.mean((actual - predicted) ** 2, axis=0)  # MSE for each strike
    MAE_test_Call = np.mean(np.abs(actual - predicted), axis=0)  # MAE for each strike
    MAPE_test_Call = np.mean(np.abs((actual - predicted) / (actual + 1e-8)), axis=0) * 100  # MAPE for each strike, avoid division by zero
    
    return MSE_test_Call, MAE_test_Call, MAPE_test_Call

# Calculate metrics for each Call option
MSE_test_Call, MAE_test_Call, MAPE_test_Call = Metrics(test_Close_prices_Call.values, test_predictions_Call)

# Print each metric for each close price in Calls
for i in range(len(MSE_test_Call)):
    print(f'Call Options Price_{i+1} - Strike_{i+1}: MSE (testing): {MSE_test_Call[i]:.4f}, MAE (testing): {MAE_test_Call[i]:.4f}, MAPE (testing): {MAPE_test_Call[i]:.2f}%')

# Create the Test_Call_options DataFrame
Test_Call_options = pd.DataFrame({
    'Exp Date = T': exp_dates_for_Call_test,
    'T - 5': test_dates_for_Call,
    '21_Day_Percent_Volatility': test_volas_for_Call_percent,
    '21_Day_Volatility': test_volas_for_Call_nominal,
    'ETF Price on T - 5': testing_Features_for_Call_options['ETF Price on T - 5'].values,
})

# Add strike prices and predictions to the DataFrame
for i in range(1, 6):  # Assuming there are 5 strike prices
    Test_Call_options[f'Strike_{i}'] = testing_Features_for_Call_options[f'Strike_{i}'].values
    Test_Call_options[f'Close_{i}'] = testing_Data_Target_Call[f'Close_{i}'].values  # Actual close prices
    Test_Call_options[f'Predicted Price_{i}'] = test_predictions_Call[:, i - 1]  # Predicted prices

# Calculate the prediction differences and percentage differences for each strike
for i in range(1, 6):
    # Calculate the difference between predicted and actual close prices
    Test_Call_options[f'Price Prediction Difference_{i}'] = Test_Call_options[f'Predicted Price_{i}'] - Test_Call_options[f'Close_{i}']
    
    # Calculate the percentage difference
    Test_Call_options[f'Price Prediction Difference in percentage_{i}'] = (
        Test_Call_options[f'Price Prediction Difference_{i}'] / Test_Call_options[f'Close_{i}'].replace(0, np.nan) * 100
    )

# Calculate average price prediction differences
Test_Call_options['Average Price Prediction Difference'] = Test_Call_options[
    [f'Price Prediction Difference_{i}' for i in range(1, 6)]
].mean(axis=1)

Test_Call_options['Average Price Prediction Difference in percentage'] = Test_Call_options[
    [f'Price Prediction Difference in percentage_{i}' for i in range(1, 6)]
].mean(axis=1)

# Overall Average Price Prediction Difference across all strikes
Test_Call_options['Overall Average Price Prediction Difference'] = Test_Call_options[
    [f'Price Prediction Difference_{i}' for i in range(1, 6)]
].mean(axis=1)

# Overall Average Price Prediction Difference in percentage across all strikes
Test_Call_options['Overall Average Price Prediction Difference in percentage'] = Test_Call_options[
    [f'Price Prediction Difference in percentage_{i}' for i in range(1, 6)]
].mean(axis=1)

# Create a DataFrame with metrics for each strike price
metrics_df = pd.DataFrame({
    'Strike': [f'Strike_{i+1} Close_{i+1}' for i in range(len(MSE_test_Call))],
    'MSE_test_Call_close': MSE_test_Call,
    'MAE_test_Call_close': MAE_test_Call,
    'MAPE_test_Call_Close': MAPE_test_Call,
})

# Assign overall model performance metrics to all rows
Test_Call_options['MSE'] = np.mean(MSE_test_Call)  # Overall MSE based on close prices
Test_Call_options['MAE'] = np.mean(MAE_test_Call)  # Overall MAE based on close prices
Test_Call_options['MAPE'] = np.mean(MAPE_test_Call)  # Overall MAPE based on close prices
#Res_Call_options['Weight decay'] = weight_decay
Res_Call_options['Learning rate'] = lr
#Res_Call_options['Dropout rate'] = dropout
Res_Call_options['Epoch size'] = Num_Epochs
Res_Call_options['Random seed'] = seed

# Print the DataFrame and metrics for verification
print(Test_Call_options.head())
print(f'MSE: {np.mean(MSE_test_Call):.4f}, MAE: {np.mean(MAE_test_Call):.4f}, MAPE: {np.mean(MAPE_test_Call):.2f}%')



# Combine the metrics with the main DataFrame
Test_Call_options = pd.concat([Test_Call_options, metrics_df], axis=1)

# Save results to Excel
with pd.ExcelWriter('./testing_results/GLD_testing_results_with_ANN_Adam_multiple_strikes_Testing_with_volatility_input_Calls_2000.xlsx') as writer:
    Test_Call_options.to_excel(writer, sheet_name='Call_Options', index=False)

# Display the final DataFrame for verification
print(Test_Call_options.head())






# Define the strike prices to plot
strikes = [1, 2, 3, 4, 5]  # Corresponding to Close price_1 to Close price_5

# Create a plot for Actual vs Predicted Close Prices for each strike
for strike in strikes:
    plt.figure(figsize=(12, 6))
    
    # Plot actual close prices
    plt.plot(range(len(Res_Call_options)), Res_Call_options[f'Close_{strike}'], 
             label=f'Actual Close Price Close_{strike} (Strike_{strike})', color='blue', marker='o', linestyle='-')
    
    # Plot predicted close prices
    plt.plot(range(len(Res_Call_options)), Res_Call_options[f'Predicted Price_{strike}'], 
             label=f'Predicted Close Price Close_{strike} (Strike_{strike})', color='red', marker='x', linestyle='--')
    
    # Add labels and title
    plt.xlabel('Day')
    plt.ylabel('Close Price in USD$')
    plt.title(f'Actual vs Predicted Close Prices ANN 8 layers 7800 Neurons  for Call Options GLD ETF Training Close_{strike} with volatility input ', fontsize=10)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    
    # Save and show the plot
    plt.savefig(f'./training_results/Actual_vs_Predicted_ANN 8 layers 7800 Neurons _Close_Prices_Close_{strike}Strike_Training{strike} with volatility input.pdf')
    plt.show()


# Create scatter plots for residuals for each strike level
for strike in strikes:
    plt.figure(figsize=(14, 6))
    plt.scatter(Res_Call_options.index, Res_Call_options[f'Price Prediction Difference_{strike}'], color='purple', alpha=0.5)
    plt.axhline(0, color='red', linestyle='--', linewidth=1)
    plt.xlabel('Day')
    plt.ylabel(f'Price Prediction Difference (Strike_{strike}) in USD$')
    plt.title(f'Residuals for Call Option Close Prices ANN 8 layers 7800 Neurons  GLD ETF Training dataset Close_{strike} with volatility input', fontsize=12)
    plt.grid(True)
    plt.tight_layout()
    
    # Save and show the residuals plot
    plt.savefig(f'./training_results/Residuals_Call_Options_ANN 8 layers 7800 Neurons _Strike_Training{strike}_Close_{strike}with volatility input.pdf')
    plt.show()

# Create a scatter plot for actual vs. predicted close prices for each strike
plt.figure(figsize=(10, 6))
for strike in strikes:
    plt.scatter(Res_Call_options[f'Close_{strike}'], Res_Call_options[f'Predicted Price_{strike}'], 
                label=f'Strike_{strike} Close_{strike}', alpha=0.5)

# Add a line of perfect prediction for reference
min_price = Res_Call_options[[f'Close_{s}' for s in strikes]].min().min()
max_price = Res_Call_options[[f'Close_{s}' for s in strikes]].max().max()
plt.plot([min_price, max_price], [min_price, max_price], color='red', linewidth=2, linestyle='--', label='Perfect Prediction Line')

# Label the scatter plot
plt.xlabel('Actual Close Price in USD$')
plt.ylabel('Predicted Close Price in USD$')
plt.title('Actual vs Predicted Close Prices for Call Options in USD$ GLD ETF ANN 8 layers 7800 Neurons  Training dataset with volatility input', fontsize=10)
plt.legend()
plt.grid(True)

# Save and show the scatter plot
plt.savefig('./training_results/Actual_vs_Predicted_Close_Prices_ANN_8 layers 7800 Neurons  Scatter_Training_with volatility input.pdf')
plt.show()

plt.figure(figsize=(12, 6))

for strike in strikes:
    plt.figure(figsize=(10, 6))  # Create a new figure for each strike
    plt.plot(dates_for_Call, Res_Call_options[f'Close_{strike}'], 
             label=f'Actual Close Price Strike_{strike}', marker='o', color='blue')
    plt.plot(dates_for_Call, Res_Call_options[f'Predicted Price_{strike}'], 
             label=f'Predicted Close Price Strike_{strike}', linestyle='--', marker='x', color='red')

    plt.xlabel('T - 5 Time years')
    plt.ylabel('Close Price in USD$')
    plt.title(f'Actual vs. Predicted Close prices GLD ETF ANN 8 layers 7800 Neurons  Over Time for Call Options Training dataset Close_{strike} with volatility input', fontsize=10)
    plt.xticks(rotation=45)  # Rotate date labels for better readability
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    # Save and show the plot for each strike
    plt.savefig(f'./training_results/Actual_vs_Predicted_Close_{strike}_8 layers 7800 Neurons  ANN_Over_Time_Strike_Training{strike}_with volatility input.pdf')
    plt.show()
    
# Set up the figure for the plots
plt.figure(figsize=(14, 10))

# Plotting Average Price Prediction Difference
plt.subplot(2, 1, 1)
plt.plot(Res_Call_options['T - 5'], Res_Call_options['Overall Average Price Prediction Difference'], 
         marker='o', linestyle='-', color='blue')
plt.xlabel('T - 5 Time years')
plt.ylabel('Average Price Prediction Difference in USD$')
plt.title('Overall Average Price Prediction Difference 8 layers 7800 Neurons  ANN GLD for GLD ETF Call Options Training dataset with volatility input', fontsize=10)
plt.grid(True)

# Plotting Average Price Prediction Difference in Percentage
plt.subplot(2, 1, 2)
plt.plot(Res_Call_options['T - 5'], Res_Call_options['Overall Average Price Prediction Difference in percentage'], 
         marker='o', linestyle='-', color='orange')
plt.xlabel('T - 5 Time years')
plt.ylabel('Average Price Prediction Difference in Percentage (%)')
plt.title('Overall Average Price Prediction Difference in Percentage 8 layers 7800 Neurons  ANN for GLD ETF Call Options Training dataset with volatility input', fontsize=10)
plt.grid(True)

# Adjust layout
plt.tight_layout()

# Save the figure
plt.savefig('./training_results/Average_Price_Prediction_Differences_Training_with volatility input_ANN_8 layers 7800 Neurons .pdf')

# Show the plots
plt.show()

 # Plot actual and predicted close prices for each strike
for strike in strikes:
    plt.plot(dates_for_Call, Res_Call_options[f'Close_{strike}'], 
             label=f'Actual Close Price Strike {strike}', marker='o')
    plt.plot(dates_for_Call, Res_Call_options[f'Predicted Price_{strike}'], 
             label=f'Predicted Close Price Close_{strike} Strike_{strike}', linestyle='--', marker='x')

plt.xlabel('T - 5 Time years')
plt.ylabel('Close Price in USD$')
plt.title('Actual vs. Predicted Close Prices Over Time for GLD ETF Call Options 8 layers 7800 Neurons  ANN Training dataset with volatility input', fontsize=10)
plt.xticks(rotation=45)  # Rotate date labels for better readability
plt.legend()
plt.grid(True)
plt.tight_layout()

# Save and show the plot
plt.savefig('./training_results/Actual_vs_Predicted_Close_Prices_Over_Time_8 layers 7800 Neurons _ANN_Training_with_volatility.pdf')
plt.show()

# Add overall price prediction difference column
Res_Call_options['Overall Price Prediction Difference'] = Res_Call_options[
    [f'Price Prediction Difference_{i}' for i in range(1, 6)]
].mean(axis=1)

# Create histograms of prediction differences for each strike
for strike in strikes:
    plt.figure(figsize=(12, 6))
    plt.hist(Res_Call_options[f'Price Prediction Difference_{strike}'], bins=30, color='green', alpha=0.7)
    plt.xlabel(f'Close_{strike} Price Prediction Difference in USD$ (Strike_{strike}) ')
    plt.ylabel('Frequency')
    plt.title(f'Histogram of Close Price Prediction Differences in USD$ Close_{strike} for GLD ETF Call Options 8 layers 7800 Neurons  ANN Training dataset with volatility input', fontsize=10)
    plt.grid(True)
    plt.tight_layout()
    
    # Save and show the histogram
    plt.savefig(f'./training_results/Prediction_Differences_Histogram_Call_Options_Close_{strike}_Strike_Training{strike}_with_volatility_input_8 layers 7800 Neurons .pdf')
    plt.show()

# Create histogram for overall prediction differences
plt.figure(figsize=(12, 6))
plt.hist(Res_Call_options['Overall Price Prediction Difference'], bins=30, color='blue', alpha=0.7)
plt.xlabel('Overall Price Prediction Difference in USD$')
plt.ylabel('Frequency')
plt.title('Histogram of Overall Prediction Differences in USD$ for GLD ETF Call Options 8 layers 7800 Neurons  ANN Training dataset with volatility input', fontsize=10)
plt.grid(True)
plt.tight_layout()

# Save and show the overall histogram
plt.savefig('./training_results/Prediction_Differences_Histogram_Call_Options_Overall_8 layers 7800 Neurons _ANN_Training_with_volatility_input.pdf')
plt.show()

# Add overall price prediction difference column
Res_Call_options['Overall Price Prediction Difference in percentage'] = Res_Call_options[
    [f'Price Prediction Difference in percentage_{i}' for i in range(1, 6)]
].mean(axis=1)

# Create histogram for overall prediction differences
plt.figure(figsize=(12, 6))
plt.hist(Res_Call_options['Overall Price Prediction Difference in percentage'], bins=30, color='blue', alpha=0.7)
plt.xlabel('Overall Price Prediction Difference in %')
plt.ylabel('Frequency')
plt.title('Histogram of Overall Prediction Differences in percentage for GLD ETF Call Options 8 layers 7800 Neurons  ANN Training dataset with volatility input', fontsize=10)
plt.grid(True)
plt.tight_layout()

# Save and show the overall histogram
plt.savefig('./training_results/Prediction_Differences_in_percentage_Histogram_Call_Options_Overall_8 layers 7800 Neurons _ANN_Training_with_volatility_input.pdf')
plt.show()






import pandas as pd
import numpy as np
import torch

# Load testing data
testing_Data_for_Call_options = pd.read_excel('test_gld_after_2016.xlsx', sheet_name='GLD_CALL')
test_volas_for_Call_percent = testing_Data_for_Call_options['21_Day_Percent_Volatility']
test_volas_for_Call_nominal = testing_Data_for_Call_options['21_Day_Volatility']

# Clean column names
testing_Data_for_Call_options.columns = testing_Data_for_Call_options.columns.str.strip()

# Prepare features and targets for Call options
testing_Features_for_Call_options = testing_Data_for_Call_options[['ETF Price on T - 5', 'Strike_1', 'Strike_2', 'Strike_3', 'Strike_4', 'Strike_5', '21_Day_Percent_Volatility', '21_Day_Volatility']].copy()
testing_Data_Target_Call = testing_Data_for_Call_options[['Close_1', 'Close_2', 'Close_3', 'Close_4', 'Close_5']]

# Convert columns to numeric
testing_Features_for_Call_options['ETF Price on T - 5'] = pd.to_numeric(testing_Features_for_Call_options['ETF Price on T - 5'], errors='coerce')

# Drop rows with missing values in the feature set
testing_Features_for_Call_options = testing_Features_for_Call_options.dropna(subset=['ETF Price on T - 5'])

# Ensure the target aligns with the filtered features
test_Close_prices_Call = testing_Data_Target_Call.loc[testing_Features_for_Call_options.index]

# Get test dates and volatility
test_dates_for_Call = testing_Data_for_Call_options['T - 5']
exp_dates_for_Call_test = testing_Data_for_Call_options['Exp Date = T']

# Drop 'Exp Date = T' column for scaling and prediction
testing_Data_for_Call_options = testing_Data_for_Call_options.drop(columns=['Exp Date = T'])



# Check if CUDA is available and set the device (reuse if defined previously)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Tensor conversion for testing features and targets, with CUDA support
Call_features_tensor_test = torch.tensor(testing_Features_for_Call_options.values, dtype=torch.float32).to(device)
Call_target_tensor_test = torch.tensor(testing_Data_Target_Call.values, dtype=torch.float32).to(device)

# Load the model and move it to the GPU
model_Call_loaded = MultiOutputOptionsPricingANN(input_features, output_strikes).to(device)
model_Call_loaded.load_state_dict(torch.load('./training_results/call_option_model.pth'))
model_Call_loaded.eval()  # Set the model to evaluation mode

# Make predictions with the loaded Call options model
with torch.no_grad():  # Disable gradient calculation
    test_predictions_Call = model_Call_loaded(Call_features_tensor_test)

# Move predictions back to CPU and convert to numpy for further processing if needed
test_predictions_Call = test_predictions_Call.cpu().numpy()

# Ensure both actual and predicted arrays have the same shape
assert Call_target_tensor_test.shape == test_predictions_Call.shape, "Shape mismatch between actual and predicted prices."

# Function to calculate metrics
def Metrics(actual, predicted):
    MSE_test_Call = np.mean((actual - predicted) ** 2, axis=0)  # MSE for each strike
    MAE_test_Call = np.mean(np.abs(actual - predicted), axis=0)  # MAE for each strike
    MAPE_test_Call = np.mean(np.abs((actual - predicted) / (actual + 1e-8)), axis=0) * 100  # MAPE for each strike, avoid division by zero
    
    return MSE_test_Call, MAE_test_Call, MAPE_test_Call

# Calculate metrics for each Call option
MSE_test_Call, MAE_test_Call, MAPE_test_Call = Metrics(test_Close_prices_Call.values, test_predictions_Call)

# Print each metric for each close price in Calls
for i in range(len(MSE_test_Call)):
    print(f'Call Options Price_{i+1} - Strike_{i+1}: MSE (testing): {MSE_test_Call[i]:.4f}, MAE (testing): {MAE_test_Call[i]:.4f}, MAPE (testing): {MAPE_test_Call[i]:.2f}%')

# Create the Test_Call_options DataFrame
Test_Call_options = pd.DataFrame({
    'Exp Date = T': exp_dates_for_Call_test,
    'T - 5': test_dates_for_Call,
    '21_Day_Percent_Volatility': test_volas_for_Call_percent,
    '21_Day_Volatility': test_volas_for_Call_nominal,
    'ETF Price on T - 5': testing_Features_for_Call_options['ETF Price on T - 5'].values,
})

# Add strike prices and predictions to the DataFrame
for i in range(1, 6):  # Assuming there are 5 strike prices
    Test_Call_options[f'Strike_{i}'] = testing_Features_for_Call_options[f'Strike_{i}'].values
    Test_Call_options[f'Close_{i}'] = testing_Data_Target_Call[f'Close_{i}'].values  # Actual close prices
    Test_Call_options[f'Predicted Price_{i}'] = test_predictions_Call[:, i - 1]  # Predicted prices

# Calculate the prediction differences and percentage differences for each strike
for i in range(1, 6):
    # Calculate the difference between predicted and actual close prices
    Test_Call_options[f'Price Prediction Difference_{i}'] = Test_Call_options[f'Predicted Price_{i}'] - Test_Call_options[f'Close_{i}']
    
    # Calculate the percentage difference
    Test_Call_options[f'Price Prediction Difference in percentage_{i}'] = (
        Test_Call_options[f'Price Prediction Difference_{i}'] / Test_Call_options[f'Close_{i}'].replace(0, np.nan) * 100
    )

# Calculate average price prediction differences
Test_Call_options['Average Price Prediction Difference'] = Test_Call_options[
    [f'Price Prediction Difference_{i}' for i in range(1, 6)]
].mean(axis=1)

Test_Call_options['Average Price Prediction Difference in percentage'] = Test_Call_options[
    [f'Price Prediction Difference in percentage_{i}' for i in range(1, 6)]
].mean(axis=1)

# Overall Average Price Prediction Difference across all strikes
Test_Call_options['Overall Average Price Prediction Difference'] = Test_Call_options[
    [f'Price Prediction Difference_{i}' for i in range(1, 6)]
].mean(axis=1)

# Overall Average Price Prediction Difference in percentage across all strikes
Test_Call_options['Overall Average Price Prediction Difference in percentage'] = Test_Call_options[
    [f'Price Prediction Difference in percentage_{i}' for i in range(1, 6)]
].mean(axis=1)

# Create a DataFrame with metrics for each strike price
metrics_df = pd.DataFrame({
    'Strike': [f'Strike_{i+1} Close_{i+1}' for i in range(len(MSE_test_Call))],
    'MSE_test_Call_close': MSE_test_Call,
    'MAE_test_Call_close': MAE_test_Call,
    'MAPE_test_Call_Close': MAPE_test_Call,
})

# Assign overall model performance metrics to all rows
Test_Call_options['MSE'] = np.mean(MSE_test_Call)  # Overall MSE based on close prices
Test_Call_options['MAE'] = np.mean(MAE_test_Call)  # Overall MAE based on close prices
Test_Call_options['MAPE'] = np.mean(MAPE_test_Call)  # Overall MAPE based on close prices
#Res_Call_options['Weight decay'] = weight_decay
Res_Call_options['Learning rate'] = lr
#Res_Call_options['Dropout rate'] = dropout
Res_Call_options['Epoch size'] = Num_Epochs
Res_Call_options['Random seed'] = seed

# Print the DataFrame and metrics for verification
print(Test_Call_options.head())
print(f'MSE: {np.mean(MSE_test_Call):.4f}, MAE: {np.mean(MAE_test_Call):.4f}, MAPE: {np.mean(MAPE_test_Call):.2f}%')



# Combine the metrics with the main DataFrame
Test_Call_options = pd.concat([Test_Call_options, metrics_df], axis=1)

# Save results to Excel
with pd.ExcelWriter('./testing_results/GLD_testing_results_with_ANN_Adam_multiple_strikes_Testing_with_volatility_input_Calls_2000.xlsx') as writer:
    Test_Call_options.to_excel(writer, sheet_name='Call_Options', index=False)

# Display the final DataFrame for verification
print(Test_Call_options.head())






# Define the strike prices to plot
strikes_test = [1, 2, 3, 4, 5]  # Corresponding to Close price_1 to Close price_5

# Create a plot for Actual vs Predicted Close Prices for each strike
for strike in strikes_test:
    plt.figure(figsize=(12, 6))
    
    # Plot actual close prices
    plt.plot(range(len(Test_Call_options)), Test_Call_options[f'Close_{strike}'], 
             label=f'Actual Close Price_{strike}(Strike_{strike})', color='blue', marker='o', linestyle='-')
    
    # Plot predicted close prices
    plt.plot(range(len(Test_Call_options)), Test_Call_options[f'Predicted Price_{strike}'], 
             label=f'Predicted Close Price_{strike} (Strike_{strike})', color='red', marker='x', linestyle='--')
    
    # Add labels and title
    plt.xlabel('Day')
    plt.ylabel('Close Price in USD$')
    plt.title(f'Actual vs Predicted Close Prices for GLD ETF Call Options ANN 8 layers 7800 Neurons  Testing dataset Close_{strike} with volatility input', fontsize=10)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    
    # Save and show the plot
    plt.savefig(f'./testing_results/Actual_vs_Predicted_Close_Prices_Close_{strike}_Testing ANN with volatility input 8 layers 7800 Neurons .pdf')
    plt.show()

# Create histograms of prediction differences for each strike
for strike in strikes_test:
    plt.figure(figsize=(12, 6))
    plt.hist(Test_Call_options[f'Price Prediction Difference_{strike}'], bins=30, color='green', alpha=0.7)
    plt.xlabel(f'Price Prediction Difference in USD$ (Strike_{strike}) Close price_{strike}')
    plt.ylabel('Frequency')
    plt.title(f'Histogram of Prediction Differences in USD$ for GLD ETF Call Options 8 layers 7800 Neurons  Testing dataset Close_{strike} with volatility input', fontsize=10)
    plt.grid(True)
    plt.tight_layout()
    
    # Save and show the histogram
    plt.savefig(f'./testing_results/Prediction_Differences_Histogram_Call_Options_ANN_Testing_Close_{strike} with volatility input.pdf')
    plt.show()

# Create scatter plots for residuals for each strike level
for strike in strikes_test:
    plt.figure(figsize=(14, 6))
    plt.scatter(Test_Call_options.index, Test_Call_options[f'Price Prediction Difference_{strike}'], color='purple', alpha=0.5)
    plt.axhline(0, color='red', linestyle='--', linewidth=1)
    plt.xlabel('Day')
    plt.ylabel(f'Price Prediction Difference in USD$ Close Price_{strike} in USD$')
    plt.title(f'Residuals for GLD ETF Call Option Close Prices Testing dataset Close_{strike} 8 layers 7800 Neurons  with volatility input', fontsize=10)
    plt.grid(True)
    plt.tight_layout()
    
    # Save and show the residuals plot
    plt.savefig(f'./testing_results/Residuals_Call_Options_Strike{strike}_Testing_Close_{strike}_ANN_with volatility input.pdf')
    plt.show()

# Create a scatter plot for actual vs. predicted close prices for each strike
plt.figure(figsize=(10, 6))
for strike in strikes_test:
    plt.scatter(Test_Call_options[f'Close_{strike}'], Test_Call_options[f'Predicted Price_{strike}'], 
                label=f'Strike_{strike} Close_{strike}', alpha=0.5)

# Add a line of perfect prediction for reference
min_price = Test_Call_options[[f'Close_{s}' for s in strikes_test]].min().min()
max_price = Test_Call_options[[f'Close_{s}' for s in strikes_test]].max().max()
plt.plot([min_price, max_price], [min_price, max_price], color='red', linewidth=2, linestyle='--', label='Perfect Prediction Line')

# Label the scatter plot
plt.xlabel('Actual Close Price in USD$')
plt.ylabel('Predicted Close Price in USD$')
plt.title('Actual vs Predicted Close Prices for GLD ETF Call Options in USD$ 8 layers 7800 Neurons  Testing dataset with volatility input', fontsize=10)
plt.legend()
plt.grid(True)

# Save and show the scatter plot
plt.savefig('./testing_results/Actual_vs_Predicted_Close_Prices_Scatter_ANN_Testing_with volatility input.pdf')
plt.show()

plt.figure(figsize=(12, 6))

for strike in strikes_test:
    plt.figure(figsize=(10, 6))  # Create a new figure for each strike
    plt.plot(test_dates_for_Call, Test_Call_options[f'Close_{strike}'], 
             label=f'Actual Close Price Strike_{strike}', marker='o', color='blue')
    plt.plot(test_dates_for_Call, Test_Call_options[f'Predicted Price_{strike}'], 
             label=f'Predicted Close Price Strike_{strike}', linestyle='--', marker='x', color='red')

    plt.xlabel('T - 5 Time years')
    plt.ylabel('Close Price in USD$')
    plt.title(f'Actual vs. Predicted Close Prices Over Time for  GLD ETF Call Options 8 layers 7800 Neurons  Testing dataset Close_{strike} with volatility input', fontsize=10)
    plt.xticks(rotation=45)  # Rotate date labels for better readability
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    # Save and show the plot for each strike
    plt.savefig(f'./testing_results/Actual_vs_Predicted_Close_Prices_Over_Time_Strike_8 layers 7800 Neurons Testing_{strike}_with volatility input.pdf')
    plt.show()
    
# Set up the figure for the plots
plt.figure(figsize=(14, 10))

# Plotting Average Price Prediction Difference
plt.subplot(2, 1, 1)
plt.plot(Test_Call_options['T - 5'], Test_Call_options['Overall Average Price Prediction Difference'], 
         marker='o', linestyle='-', color='blue')
plt.xlabel('T - 5 Time years')
plt.ylabel('Average Price Prediction Difference in USD$')
plt.title('Overall Average Price Prediction Difference for Call Options 8 layers 7800 Neurons  Testing dataset with volatility input', fontsize=14)
plt.grid(True)

# Plotting Average Price Prediction Difference in Percentage
plt.subplot(2, 1, 2)
plt.plot(Test_Call_options['T - 5'], Test_Call_options['Overall Average Price Prediction Difference in percentage'], 
         marker='o', linestyle='-', color='orange')
plt.xlabel('T - 5 Time in years')
plt.ylabel('Average Price Prediction Difference in Percentage (%)')
plt.title('Overall Average Price Prediction Difference in Percentage for GLD ETF Call Options 8 layers 7800 Neurons  Testing dataset with volatility input', fontsize=14)
plt.grid(True)

# Adjust layout
plt.tight_layout()

# Save the figure
plt.savefig('./testing_results/Average_Price_Prediction_Differences_ 8 layers 7800 Neurons Testing_with volatility input.pdf')

# Show the plots
plt.show()

# Add overall price prediction difference column
Test_Call_options['Overall Price Prediction Difference'] = Test_Call_options[
    [f'Price Prediction Difference_{i}' for i in range(1, 6)]
].mean(axis=1)



# Create histogram for overall prediction differences
plt.figure(figsize=(12, 6))
plt.hist(Test_Call_options['Overall Price Prediction Difference'], bins=30, color='blue', alpha=0.7)
plt.xlabel('Overall Price Prediction Difference in USD$')
plt.ylabel('Frequency')
plt.title('Histogram of Overall Prediction Differences in USD$ for GLD ETF Call Options 8 layers 7800 Neurons  Testing dataset with volatility input', fontsize=10)
plt.grid(True)
plt.tight_layout()

# Save and show the overall histogram
plt.savefig('./testing_results/Prediction_Differences_Histogram_Call_Options_Overall_8 layers 7800 Neurons _Testing_with_volatility_input.pdf')
plt.show()




# Add overall price prediction difference column
Res_Call_options['Overall Price Prediction Difference in percentage'] = Res_Call_options[
    [f'Price Prediction Difference_{i}' for i in range(1, 6)]
].mean(axis=1)

# Create histogram for overall prediction differences
plt.figure(figsize=(12, 6))
plt.hist(Res_Call_options['Overall Price Prediction Difference in percentage'], bins=30, color='blue', alpha=0.7)
plt.xlabel('Overall Price Prediction Difference %')
plt.ylabel('Frequency')
plt.title('Histogram of Overall Prediction Differences in percentage for GLD ETF Call Options 8 layers 7800 Neurons  Testing dataset with volatility input', fontsize=10)
plt.grid(True)
plt.tight_layout()

# Save and show the overall histogram
plt.savefig('./training_results/Prediction_Differences_Histogram_Call_in_percentage_Options_Overall_ANN_Testing_with_volatility_input.pdf')
plt.show()
